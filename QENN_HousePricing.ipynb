{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz6yu0aM/M21eo3H9pWjsi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RabiyaAkhtar/HousePricingPrediction-Quantile-ML/blob/main/QENN_HousePricing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YtaXjS-EIsc"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "import pandas as pd\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== [1. Data Loading & Preprocessing] ======================\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
        "\n",
        "# Log-transform targets\n",
        "train_targets = np.log(train_targets)\n",
        "test_targets = np.log(test_targets)\n",
        "\n",
        "# Normalize features\n",
        "mean = train_data.mean(axis=0)\n",
        "std = train_data.std(axis=0)\n",
        "train_data = (train_data - mean) / std\n",
        "test_data = (test_data - mean) / std"
      ],
      "metadata": {
        "id": "9ufEkfAxbHCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== [2. Feature Engineering] ======================\n",
        "\n",
        "# Polynomial features\n",
        "poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
        "train_data_poly = poly.fit_transform(train_data)\n",
        "test_data_poly = poly.transform(test_data)\n",
        "\n",
        "# Normalize polynomial features\n",
        "mean_poly = train_data_poly.mean(axis=0)\n",
        "std_poly = train_data_poly.std(axis=0)\n",
        "train_data_poly = (train_data_poly - mean_poly) / std_poly\n",
        "test_data_poly = (test_data_poly - mean_poly) / std_poly\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(mutual_info_regression, k=15)\n",
        "train_data_selected = selector.fit_transform(train_data_poly, train_targets)\n",
        "test_data_selected = selector.transform(test_data_poly)\n"
      ],
      "metadata": {
        "id": "Ua_LhJVUbDkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== [3. Model Architecture] ======================\n",
        "\n",
        "def build_enhanced_model(input_shape):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    attention = layers.Dense(input_shape[0], activation='softmax')(inputs)\n",
        "    attended = layers.Multiply()([inputs, attention])\n",
        "\n",
        "    x = layers.Dense(64, activation='swish')(attended)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    x_res = layers.Dense(64, activation='swish')(x)\n",
        "    x_res = layers.BatchNormalization()(x_res)\n",
        "    x = layers.Add()([x, x_res])\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Dense(32, activation='swish')(x)\n",
        "    outputs = layers.Dense(1)(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "AZvitZu_a9zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== [4. Model Training] ======================\n",
        "quantiles = [0.05, 0.5, 0.95]\n",
        "quantile_models = {}\n",
        "\n",
        "for q in quantiles:\n",
        "    print(f\" --> Training quantile {q} model...\")\n",
        "    model = build_enhanced_model((train_data_selected.shape[1],))\n",
        "\n",
        "    def quantile_loss(y_true, y_pred):\n",
        "        err = y_true - y_pred\n",
        "        return keras.backend.mean(keras.backend.maximum(q*err, (q-1)*err))\n",
        "\n",
        "    model.compile(optimizer='nadam', loss=quantile_loss)\n",
        "    model.fit(train_data_selected, train_targets, epochs=150, batch_size=16, verbose=0)\n",
        "    quantile_models[q] = model"
      ],
      "metadata": {
        "id": "mqZiByI0a5H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== [5. Advanced Analysis] ======================\n",
        "\n",
        "# --- Improving predictions by correcting residual errors on test set ---\n",
        "\n",
        "# Median predictions needed for residual correction.\n",
        "median_pred = quantile_models[0.5].predict(test_data_selected).flatten()\n",
        "\n",
        "residuals = test_targets - median_pred\n",
        "residual_model = build_enhanced_model((test_data_selected.shape[1],))\n",
        "residual_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mae',\n",
        "    metrics=['mae']\n",
        ")\n",
        "residual_model.fit(test_data_selected, residuals, epochs=100, batch_size=16, verbose=0)\n",
        "corrected_preds = median_pred + residual_model.predict(test_data_selected).flatten()\n",
        "corrected_mae = np.mean(np.abs(test_targets - corrected_preds))\n",
        "print(f\"Corrected MAE: {corrected_mae:.4f}\")\n",
        "\n",
        "# Difficult cases handling\n",
        "train_preds = quantile_models[0.5].predict(train_data_selected).flatten()  # Explicitly using median model\n",
        "train_errors = np.abs(train_targets - train_preds)\n",
        "threshold = np.percentile(train_errors, 75)\n",
        "difficult_mask = train_errors > threshold\n",
        "difficult_data = train_data_selected[difficult_mask]\n",
        "difficult_targets = train_targets[difficult_mask]\n",
        "\n",
        "if len(difficult_targets) > 0:\n",
        "    specialized_model = build_enhanced_model((train_data_selected.shape[1],))\n",
        "    specialized_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss='mae',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "    specialized_model.fit(\n",
        "        difficult_data,\n",
        "        difficult_targets,\n",
        "        epochs=200,\n",
        "        batch_size=16,\n",
        "        verbose=0,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    test_preds = quantile_models[0.5].predict(test_data_selected).flatten()\n",
        "    test_errors = np.abs(test_targets - test_preds)\n",
        "    difficult_test_mask = test_errors > 2.5\n",
        "\n",
        "    if np.any(difficult_test_mask):\n",
        "        difficult_test_data = test_data_selected[difficult_test_mask]\n",
        "        improved_preds = test_preds.copy()\n",
        "        improved_preds[difficult_test_mask] = (\n",
        "            0.7 * specialized_model.predict(difficult_test_data).flatten() +\n",
        "            0.3 * test_preds[difficult_test_mask]\n",
        "        )\n",
        "        final_mae = np.mean(np.abs(test_targets - improved_preds))\n",
        "        print(f\"MAE after handling difficult cases: {final_mae:.4f}\")\n",
        "    else:\n",
        "        print(\"No difficult cases found in test set.\")\n",
        "else:\n",
        "    print(\"No difficult cases found in training set.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "e3NVdEKvZh8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== [6. Compiling Results] ======================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Predict quantiles\n",
        "low_pred = quantile_models[0.05].predict(test_data_selected).flatten()\n",
        "high_pred = quantile_models[0.95].predict(test_data_selected).flatten()\n",
        "median_pred = quantile_models[0.5].predict(test_data_selected).flatten()\n",
        "\n",
        "# Final weighted prediction\n",
        "final_preds = (0.2 * low_pred + 0.6 * median_pred + 0.2 * high_pred)\n",
        "\n",
        "# Defining prediction intervals\n",
        "lower_bound = np.minimum(low_pred, high_pred)\n",
        "upper_bound = np.maximum(low_pred, high_pred)\n",
        "\n",
        "# Results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Actual_Price': test_targets,\n",
        "    'Predicted_Price': final_preds,\n",
        "    'Lower_Bound': lower_bound,\n",
        "    'Upper_Bound': upper_bound,\n",
        "    'Error': test_targets - final_preds,\n",
        "    'Absolute_Error': np.abs(test_targets - final_preds),\n",
        "    'Percent_Error': (np.abs(test_targets - final_preds) / test_targets) * 100,\n",
        "    'Within_CI': (test_targets >= lower_bound) & (test_targets <= upper_bound)\n",
        "})\n",
        "\n",
        "# Adding more metrics\n",
        "results_df['Error_Rank'] = results_df['Absolute_Error'].rank(ascending=False)\n",
        "results_df['Prediction_Interval_Width'] = results_df['Upper_Bound'] - results_df['Lower_Bound']\n",
        "\n",
        "# Formating display\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "# Previewing top 10 rows\n",
        "print(\"\\n Top 10 Predictions:\")\n",
        "print(results_df[['Actual_Price', 'Predicted_Price', 'Lower_Bound', 'Upper_Bound',\n",
        "                  'Error', 'Absolute_Error', 'Percent_Error', 'Within_CI']].head(10))\n",
        "\n",
        "# Summary statis:\n",
        "summary_stats = pd.DataFrame({\n",
        "    'Mean': results_df.mean(),\n",
        "    'Median': results_df.median(),\n",
        "    'Std': results_df.std(),\n",
        "    'Min': results_df.min(),\n",
        "    'Max': results_df.max()\n",
        "})\n",
        "\n",
        "print(\"\\n Summary Statistics:\")\n",
        "print(summary_stats.loc[['Actual_Price', 'Predicted_Price', 'Error', 'Absolute_Error',\n",
        "                         'Percent_Error', 'Prediction_Interval_Width']])\n",
        "\n",
        "# CI coverage\n",
        "coverage = results_df['Within_CI'].mean() * 100\n",
        "print(f\"\\n Coverage: {coverage:.1f}% of actual prices fall within the prediction intervals\")\n"
      ],
      "metadata": {
        "id": "80lyzGbaXo0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== [7. Plot Visualization] ======================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "# Plot 1: Actual vs Predicted\n",
        "axes[0].scatter(test_targets, final_preds)\n",
        "axes[0].set_xlabel(\"Actual Price\")\n",
        "axes[0].set_ylabel(\"Predicted Price\")\n",
        "axes[0].set_title(\"Actual vs. Predicted Housing Prices\")\n",
        "axes[0].plot([min(test_targets), max(test_targets)], [min(test_targets), max(test_targets)], 'r--')\n",
        "\n",
        "# Plot 2: Actual vs Predicted with Confidence Intervals\n",
        "\n",
        "lower_error = final_preds - low_pred\n",
        "upper_error = high_pred - final_preds\n",
        "\n",
        "# Ensuring no negative values in error bars\n",
        "lower_error = np.abs(lower_error)\n",
        "upper_error = np.abs(upper_error)\n",
        "\n",
        "# Ploting with error bars\n",
        "axes[1].errorbar(test_targets, final_preds,\n",
        "                 yerr=[lower_error, upper_error],\n",
        "                 fmt='o', alpha=0.7,\n",
        "                 capsize=5, capthick=2,\n",
        "                 label='Predictions with 80% CI')\n",
        "\n",
        "# Perfect prediction line\n",
        "axes[1].plot([min(test_targets), max(test_targets)],\n",
        "             [min(test_targets), max(test_targets)],\n",
        "             'r--', label='Perfect Prediction')\n",
        "\n",
        "\n",
        "axes[1].set_xlabel('Actual Prices ($1000s)', fontsize=12)\n",
        "axes[1].set_ylabel('Predicted Prices ($1000s)', fontsize=12)\n",
        "axes[1].set_title('Actual vs Predicted Prices with Confidence Intervals', fontsize=14)\n",
        "axes[1].grid(True, linestyle='--', alpha=0.7)\n",
        "axes[1].legend(fontsize=10)\n",
        "\n",
        "\n",
        "corr = np.corrcoef(test_targets, final_preds)[0, 1]\n",
        "axes[1].text(0.05, 0.9, f'Pearson r = {corr:.3f}',\n",
        "             transform=axes[1].transAxes,\n",
        "             fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3-uPGpk_FSDs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== [8. Interactive Prediction] ======================\n",
        "# Predict using an index from the test set and see actual price too\n",
        "\n",
        "from ipywidgets import IntSlider, interact\n",
        "import random\n",
        "\n",
        "def test_set_prediction(idx):\n",
        "    print(f\"\\n Example #{idx + 1} from test set:\")\n",
        "    actual = test_targets[idx]\n",
        "    pred = final_preds[idx]\n",
        "    low = lower_bound[idx]\n",
        "    high = upper_bound[idx]\n",
        "    error = pred - actual\n",
        "\n",
        "    print(f\"   Actual Price (log scale): {actual:.2f}\")\n",
        "    print(f\"   Predicted Price (log scale): {pred:.2f}\")\n",
        "    print(f\"   Prediction Interval: [{low:.2f}, {high:.2f}]\")\n",
        "    print(f\"   Error: {error:.2f}\")\n",
        "    print(\" This shows how well the model performs on real examples from the dataset.\")\n",
        "\n",
        "# Ask the user first\n",
        "try:\n",
        "    user_input = input(\"\\n Would you like to test the model on a random example from test data? (yes/no): \").strip().lower()\n",
        "\n",
        "    if user_input in ['yes', 'y']:\n",
        "        print(\"\\n Use the slider below to test the model on a specific test example:\")\n",
        "        interact(test_set_prediction, idx=IntSlider(min=0, max=len(test_targets)-1, step=1, description='Test Index'))\n",
        "\n",
        "    else:\n",
        "        print(\"\\n Alright! You can always re-run this cell to test later.\")\n",
        "except:\n",
        "    print(\" Note: Interactive input may not work in some Colab environments (e.g., if using notebook viewer).\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VV5HfoJTTjzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================ [9. Hypothetical Interactive Prediction] =================\n",
        "\n",
        "# Hypothetical prediction with feature names same as in dataset used\n",
        "\n",
        "# Use original (non-normalized) train data for realistic min/max\n",
        "original_train, _ = boston_housing.load_data()\n",
        "original_train_data = original_train[0]\n",
        "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
        "                 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "\n",
        "# Get min/max for sliders\n",
        "min_vals = original_train_data.min(axis=0)\n",
        "max_vals = original_train_data.max(axis=0)\n",
        "\n",
        "# Building sliders dynamically\n",
        "sliders = {}\n",
        "for i, name in enumerate(feature_names):\n",
        "    sliders[name] = widgets.FloatSlider(\n",
        "        value=(min_vals[i] + max_vals[i]) / 2,\n",
        "        min=min_vals[i],\n",
        "        max=max_vals[i],\n",
        "        step=(max_vals[i] - min_vals[i]) / 100,\n",
        "        description=name,\n",
        "        continuous_update=False,\n",
        "        layout=widgets.Layout(width='40%')\n",
        "    )\n",
        "\n",
        "# Preprocessing and prediction function\n",
        "def predict_house_price(**kwargs):\n",
        "    user_input = np.array([kwargs[name] for name in feature_names]).reshape(1, -1)\n",
        "\n",
        "    # Normalize like original train\n",
        "    user_input_norm = (user_input - mean) / std\n",
        "\n",
        "    # Polynomial transform and normalize\n",
        "    user_poly = poly.transform(user_input_norm)\n",
        "    user_poly = (user_poly - mean_poly) / std_poly\n",
        "\n",
        "    # Feature select\n",
        "    user_selected = selector.transform(user_poly)\n",
        "\n",
        "    # Quantile predictions\n",
        "    low = quantile_models[0.05].predict(user_selected).flatten()[0]\n",
        "    median = quantile_models[0.5].predict(user_selected).flatten()[0]\n",
        "    high = quantile_models[0.95].predict(user_selected).flatten()[0]\n",
        "\n",
        "    # Combine with residual correction\n",
        "    correction = residual_model.predict(user_selected).flatten()[0]\n",
        "    corrected = median + correction\n",
        "\n",
        "    # Weighted final prediction\n",
        "    final = 0.2 * low + 0.6 * median + 0.2 * high\n",
        "\n",
        "    # Undo log-transform\n",
        "    print(\"\\n *Model performs best for houses similar to those in our dataset.*\")\n",
        "    print(\"\\n Following prediction is for a hypothetical house in Boston and has no actual price.\")\n",
        "    print(f\"\\n Predicted House Price: ${np.exp(final):,.2f}\")\n",
        "    print(f\" Lower Bound: ${np.exp(low):,.2f}\")\n",
        "    print(f\" Upper Bound: ${np.exp(high):,.2f}\")\n",
        "\n",
        "# Display interactive sliders\n",
        "interactive_ui = interactive(predict_house_price, **sliders)\n",
        "display(interactive_ui)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3ls_zjH1tdEy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}